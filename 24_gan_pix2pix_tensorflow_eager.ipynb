{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z-arabi/SRU-deeplearning-workshop/blob/master/24_gan_pix2pix_tensorflow_eager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vejff3HnL3EU"
      },
      "source": [
        "# pix2pix\n",
        "\n",
        "Code Source\n",
        "\n",
        "[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITZuApL56Mny"
      },
      "source": [
        "This notebook demonstrates image to image translation using conditional GAN's, as described in [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004). Using this technique we can colorize black and white photos, convert google maps to google earth, etc. Here, we convert building facades to real buildings. We use [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager) to achieve this.\n",
        "\n",
        "In example, we will use the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/), helpfully provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep our example short, we will use a preprocessed [copy](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/) of this dataset, created by the authors of the [paper](https://arxiv.org/abs/1611.07004) above.\n",
        "\n",
        "Each epoch takes around 58 seconds on a single P100 GPU.\n",
        "\n",
        "Below is the output generated after training the model for 200 epochs.\n",
        "\n",
        "\n",
        "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
        "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Import TensorFlow and enable eager execution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet tensorflow==2.8"
      ],
      "metadata": {
        "id": "PbprvrH4xI3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "outputs": [],
      "source": [
        "# eager execution is enabled for TensorFlow >= 2.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "You can download this dataset and similar datasets from [here](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets). As mentioned in the [paper](https://arxiv.org/abs/1611.07004) we apply random jittering and mirroring to the training dataset.\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right.  \n",
        "\n",
        "Jittering and mirroring are data augmentation techniques.  \n",
        "* In random jittering, an image is first resized to a larger dimension (e.g., 286 x 286) and then randomly cropped to its original dimension (e.g., 256 x 256).   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn-k8kTXuAlv"
      },
      "outputs": [],
      "source": [
        "path_to_zip = tf.keras.utils.get_file('facades.tar.gz',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz',\n",
        "                                      extract=True)\n",
        "\n",
        "print(path_to_zip, os.path.dirname(path_to_zip))\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'facades/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = os.path.join(PATH,\"train\")\n",
        "train_images = os.listdir(train_path)\n",
        "train_images = [os.path.join(train_path, i) for i in train_images]\n",
        "\n",
        "img_sample = plt.imread(train_images[0])\n",
        "plt.imshow(img_sample)"
      ],
      "metadata": {
        "id": "iR_-c0m89tSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_sample.min(), img_sample.max()"
      ],
      "metadata": {
        "id": "6rlXST5K_40T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(img_sample.shape)\n",
        "\n",
        "fig, axes = plt.subplots(1,2)\n",
        "\n",
        "axes[0].imshow(img_sample[:,:256,:])\n",
        "axes[0].set_title(\"the ground truth\")\n",
        "axes[1].imshow(img_sample[:,256:,:])\n",
        "axes[1].set_title(\"the input\")"
      ],
      "metadata": {
        "id": "MUEWnVXJ-jm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_images))\n",
        "\n",
        "test_path = os.path.join(PATH,\"test\")\n",
        "test_images = os.listdir(test_path)\n",
        "print(len(test_images))\n",
        "\n",
        "val_path = os.path.join(PATH,\"val\")\n",
        "val_images = os.listdir(val_path)\n",
        "print(len(val_images))"
      ],
      "metadata": {
        "id": "Q9b2FQr__eXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CbTEt448b4R"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how to resize the picture, all possible answers:\n",
        "\n",
        "import cv2\n",
        "# read BGR\n",
        "image = cv2.imread(train_images[0])\n",
        "print(\"the cv2 reading\", type(image))\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "plt.figure()\n",
        "plt.imshow(image[:,:,-1])\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.figure()\n",
        "plt.imshow(image_rgb)\n",
        "resized_image = cv2.resize(image, (200, 200))\n",
        "plt.figure()\n",
        "plt.imshow(resized_image)\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "image = Image.open(train_images[0])\n",
        "print(\"the PIL reading\", type(image))\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "resized_image = image.resize((200, 200))\n",
        "\n",
        "\n",
        "from skimage.transform import resize\n",
        "import imageio\n",
        "image = imageio.imread(train_images[0])\n",
        "print(\"the sklearn reading\", type(image))\n",
        "resized_image = resize(image, (200, 200))\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "image = tf.io.read_file(train_images[0])\n",
        "print(\"the tf reading\", type(image))\n",
        "image = tf.image.decode_jpeg(image)\n",
        "print(\"the tf reading\", type(image))\n",
        "plt.figure()\n",
        "plt.imshow(image)\n",
        "# it returns the float points between 0 to 255\n",
        "resized_image = tf.image.resize(image, [200, 200])\n",
        "plt.figure()\n",
        "plt.imshow(resized_image)\n",
        "# plt imshow needs the int to show\n",
        "resized_image = tf.cast(resized_image, tf.uint8)\n",
        "plt.figure()\n",
        "plt.imshow(resized_image)\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Resize, ToTensor, ToPILImage\n",
        "image = Image.open(train_images[0])\n",
        "to_tensor = ToTensor() # convert to pytorh tensor\n",
        "image = to_tensor(image)\n",
        "print(\"the pytorch reading\", type(image))\n",
        "resize = Resize((200, 200))\n",
        "image = resize(image)\n",
        "to_pil_image = ToPILImage()\n",
        "image = to_pil_image(image)"
      ],
      "metadata": {
        "id": "AatKuVjdAh15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyaP4hLJ8b4W"
      },
      "outputs": [],
      "source": [
        "def load_image(image_file, is_train):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "\n",
        "  w = tf.shape(image)[1]\n",
        "\n",
        "  w = w // 2\n",
        "  real_image = image[:, :w, :]\n",
        "  input_image = image[:, w:, :]\n",
        "\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  if is_train:\n",
        "    # random jittering\n",
        "\n",
        "    # resizing to 286 x 286 x 3 > for both input and labels\n",
        "    # When this flag is set to True, the centers of the corner pixels of the input and output tensors are aligned.\n",
        "    # This preserves the values at the corner pixels.\n",
        "    input_image = tf.image.resize(input_image, [286, 286],\n",
        "                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_image = tf.image.resize(real_image, [286, 286],\n",
        "                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    # randomly cropping to 256 x 256 x 3\n",
        "    # This stacks tensors along a new axis (axis=0), essentially creating a new tensor that has an additional dimension.\n",
        "    # The new shape will be [2, height, width, channels]\n",
        "    # because we want the input and label simultanously crops together we stack them\n",
        "    stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "    cropped_image = tf.image.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "    input_image, real_image = cropped_image[0], cropped_image[1]\n",
        "\n",
        "    if np.random.random() > 0.5:\n",
        "      # random mirroring\n",
        "      input_image = tf.image.flip_left_right(input_image)\n",
        "      real_image = tf.image.flip_left_right(real_image)\n",
        "  else:\n",
        "    input_image = tf.image.resize(input_image, size=[IMG_HEIGHT, IMG_WIDTH],\n",
        "                                         method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    real_image = tf.image.resize(real_image, size=[IMG_HEIGHT, IMG_WIDTH],\n",
        "                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  # normalizing the images to [-1, 1]\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGN6ouoQxt3"
      },
      "source": [
        "## Use tf.data to create batches, map(do preprocessing) and shuffle the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQHmYSmk8b4b"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.map(lambda x: load_image(x, True))\n",
        "# each batch will only contain one image\n",
        "train_dataset = train_dataset.batch(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_iterator = iter(train_dataset)\n",
        "first_batch = next(dataset_iterator)\n",
        "print(\"First batch shape:\", first_batch[0].shape, first_batch[1].shape)"
      ],
      "metadata": {
        "id": "BiB43_AEN8Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for batch in train_dataset:\n",
        "    c += 1\n",
        "print(\"the whole data size: \", c)"
      ],
      "metadata": {
        "id": "TVOsNmrSHvZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS9J0yA58b4g"
      },
      "outputs": [],
      "source": [
        "test_dataset = tf.data.Dataset.list_files(PATH+'test/*.jpg')\n",
        "test_dataset = test_dataset.map(lambda x: load_image(x, False))\n",
        "test_dataset = test_dataset.batch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Write the generator and discriminator models\n",
        "\n",
        "* **Generator**\n",
        "  * The architecture of generator is a modified U-Net.\n",
        "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
        "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
        "  * There are skip connections between the encoder and decoder (as in U-Net). [Skip connections are used to pass information directly from the encoder to the decoder to assist in better reconstruction.]\n",
        "  \n",
        "* **Discriminator**\n",
        "  * The Discriminator is a PatchGAN. [This is a special type of discriminator that classifies whether each patch in an image is real or fake. It doesn't classify the entire image as a whole but instead operates on patches of the image.]\n",
        "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
        "  * The shape of the output after the last layer is (batch_size, 30, 30, 1) [ 1 number or 1 bool]\n",
        "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).   \n",
        "  The ouput ill be grided into 30x30. Each cell represent the 70x70 pixels of the input image. In PatchGAN, the 70x70 patches from the 256x256 input image overlap with each other. This means that adjacent patches share a significant number of pixels. This overlapping allows the discriminator to have a more extensive view of each portion of the input image, and it does not need to have a unique output cell for every unique 70x70 patch.  \n",
        "  When you obtain a 30x30 grid, each of these 900 cells is making a \"vote\" based on its own 70x70 patch. Since these patches overlap, collectively they provide a more nuanced interpretation of the 256x256 input image. [like the f=70 and s=5]\n",
        "  * Discriminator receives 2 inputs.\n",
        "    * Input image and the target image, which it should classify as real.\n",
        "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
        "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)\n",
        "\n",
        "* Shape of the input travelling through the generator and the discriminator is in the comments in the code.\n",
        "\n",
        "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004).\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqqvWxlw8b4l"
      },
      "outputs": [],
      "source": [
        "OUTPUT_CHANNELS = 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Downsample(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, filters, size, apply_batchnorm=True):\n",
        "    super(Downsample, self).__init__()\n",
        "    self.apply_batchnorm = apply_batchnorm\n",
        "    # mean and std\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters,\n",
        "                                        (size, size),\n",
        "                                        strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False)\n",
        "    if self.apply_batchnorm:\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self, x, training):\n",
        "    x = self.conv1(x)\n",
        "    if self.apply_batchnorm:\n",
        "        # The training parameter is often included in the call method of custom\n",
        "        # Keras layers or models to specify whether the model is in training mode or\n",
        "        # inference mode. This is particularly important for layers like BatchNormalization\n",
        "        # and Dropout, which have different behaviors during training and inference.\n",
        "        # During training, this layer normalizes its output using the mean and standard\n",
        "        # deviation of the current batch of inputs. During inference, it uses a running\n",
        "        # average of the mean and standard deviation it has learned during training.\n",
        "        # This layer randomly sets a fraction of its input units to 0 during training,\n",
        "        # which helps to prevent overfitting. During inference, it does nothing.\n",
        "        x = self.batchnorm(x, training=training)\n",
        "\n",
        "    # When encoding or downsampling information, it might be beneficial to preserve as much information as possible,\n",
        "    # including the sign of the data.\n",
        "    # Leaky ReLU allows a small, non-zero gradient when the input is less than zero.\n",
        "    # This can help mitigate the \"dying ReLU problem,\" where neurons can sometimes get stuck during training and always output zero.\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Upsample(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, filters, size, apply_dropout=False):\n",
        "    super(Upsample, self).__init__()\n",
        "    self.apply_dropout = apply_dropout\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    self.up_conv = tf.keras.layers.Conv2DTranspose(filters,\n",
        "                                                   (size, size),\n",
        "                                                   strides=2,\n",
        "                                                   padding='same',\n",
        "                                                   kernel_initializer=initializer,\n",
        "                                                   use_bias=False)\n",
        "    self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "    if self.apply_dropout:\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "  def call(self, x1, x2, training):\n",
        "    x = self.up_conv(x1)\n",
        "    x = self.batchnorm(x, training=training)\n",
        "    if self.apply_dropout:\n",
        "        x = self.dropout(x, training=training)\n",
        "    # ReLU activation leads to sparse representations. Sparsity is often beneficial\n",
        "    # because it can make the network easier to optimize and can lead to a more expressive model.\n",
        "    # In many architectures like U-Net, ReLU is commonly used in the decoder (upsampling) part.\n",
        "    x = tf.nn.relu(x)\n",
        "    # the skip connections > to not forget the information we have some residual connections\n",
        "    x = tf.concat([x, x2], axis=-1)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Generator(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    # the filter numbers shows the depth > must be growing\n",
        "    self.down1 = Downsample(64, 4, apply_batchnorm=False)\n",
        "    self.down2 = Downsample(128, 4)\n",
        "    self.down3 = Downsample(256, 4)\n",
        "    self.down4 = Downsample(512, 4)\n",
        "    self.down5 = Downsample(512, 4)\n",
        "    self.down6 = Downsample(512, 4)\n",
        "    self.down7 = Downsample(512, 4)\n",
        "    self.down8 = Downsample(512, 4)\n",
        "\n",
        "    self.up1 = Upsample(512, 4, apply_dropout=True)\n",
        "    self.up2 = Upsample(512, 4, apply_dropout=True)\n",
        "    self.up3 = Upsample(512, 4, apply_dropout=True)\n",
        "    self.up4 = Upsample(512, 4)\n",
        "    self.up5 = Upsample(256, 4)\n",
        "    self.up6 = Upsample(128, 4)\n",
        "    self.up7 = Upsample(64, 4)\n",
        "\n",
        "    # OUTPUT_CHANNELS > 3\n",
        "    self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS,\n",
        "                                                (4, 4),\n",
        "                                                strides=2,\n",
        "                                                padding='same',\n",
        "                                                kernel_initializer=initializer)\n",
        "\n",
        "  def call(self, x, training):\n",
        "    # x shape == (bs, 256, 256, 3)\n",
        "    x1 = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
        "    x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)\n",
        "    x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)\n",
        "    x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)\n",
        "    x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)\n",
        "    x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)\n",
        "    x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)\n",
        "    x8 = self.down8(x7, training=training) # (bs, 1, 1, 512) > that is the latent size\n",
        "\n",
        "    x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024) > concat > 512*2\n",
        "    x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)\n",
        "    x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)\n",
        "    x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)\n",
        "    x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)\n",
        "    x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)\n",
        "    x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)\n",
        "\n",
        "    x16 = self.last(x15) # (bs, 256, 256, 3)\n",
        "    # We normalized the input to be between 0 and 1, so it is good activation function for the output\n",
        "    x16 = tf.nn.tanh(x16)\n",
        "\n",
        "    return x16"
      ],
      "metadata": {
        "id": "vg2yc4cmyEvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscDownsample(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, filters, size, apply_batchnorm=True):\n",
        "    super(DiscDownsample, self).__init__()\n",
        "    self.apply_batchnorm = apply_batchnorm\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters,\n",
        "                                        (size, size),\n",
        "                                        strides=2,\n",
        "                                        padding='same',\n",
        "                                        kernel_initializer=initializer,\n",
        "                                        use_bias=False)\n",
        "    if self.apply_batchnorm:\n",
        "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "  def call(self, x, training):\n",
        "    x = self.conv1(x)\n",
        "    if self.apply_batchnorm:\n",
        "        x = self.batchnorm(x, training=training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "    return x\n",
        "\n",
        "class Discriminator(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    self.down1 = DiscDownsample(64, 4, False)\n",
        "    self.down2 = DiscDownsample(128, 4)\n",
        "    self.down3 = DiscDownsample(256, 4)\n",
        "    # we are zero padding here with 1 because we need our shape to\n",
        "    # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n",
        "    self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n",
        "    self.conv = tf.keras.layers.Conv2D(512,\n",
        "                                       (4, 4),\n",
        "                                       strides=1,\n",
        "                                       kernel_initializer=initializer,\n",
        "                                       use_bias=False)\n",
        "    self.batchnorm1 = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n",
        "    self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n",
        "    self.last = tf.keras.layers.Conv2D(1,\n",
        "                                       (4, 4),\n",
        "                                       strides=1,\n",
        "                                       kernel_initializer=initializer)\n",
        "\n",
        "  def call(self, inp, tar, training):\n",
        "    # concatenating the input and the target [real image]\n",
        "    x = tf.concat([inp, tar], axis=-1) # (bs, 256, 256, channels*2) > channels=3\n",
        "    x = self.down1(x, training=training) # (bs, 128, 128, 64)\n",
        "    x = self.down2(x, training=training) # (bs, 64, 64, 128)\n",
        "    x = self.down3(x, training=training) # (bs, 32, 32, 256)\n",
        "\n",
        "    x = self.zero_pad1(x) # (bs, 34, 34, 256)\n",
        "    x = self.conv(x)      # (bs, 31, 31, 512)\n",
        "    x = self.batchnorm1(x, training=training)\n",
        "    x = tf.nn.leaky_relu(x)\n",
        "\n",
        "    x = self.zero_pad2(x) # (bs, 33, 33, 512)\n",
        "    # don't add a sigmoid activation here since\n",
        "    # the loss function expects raw logits.\n",
        "    x = self.last(x)      # (bs, 30, 30, 1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "v_IWlUxVyOpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDkA05NE6QMs"
      },
      "outputs": [],
      "source": [
        "# The call function of Generator and Discriminator have been decorated\n",
        "# with tf.contrib.eager.defun()\n",
        "# We get a performance speedup if defun is used (~25 seconds per epoch)\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# where is mentioned that the patch is 70*70 ??????"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Receptive Field Calculation\n",
        "\n",
        "To calculate the effective receptive field, we work our way from the output layer back to the input layer. The formula for calculating the receptive field $ \\text{RF} $ is:\n",
        "\n",
        "$$\n",
        "\\text{RF} = \\text{RF}_{\\text{prev}} + (\\text{Kernel Size} - 1) \\times \\text{Stride}_{\\text{prod}}\n",
        "$$\n",
        "\n",
        "Where $ \\text{RF}_{\\text{prev}} $ is the receptive field of the previous layer and $ \\text{Stride}_{\\text{prod}} $ is the product of the strides of all preceding layers.\n",
        "\n",
        "For this architecture, assuming all Conv2D and Conv2DTranspose layers use 4x4 kernels, let's calculate:\n",
        "\n",
        "1. **Last Layer (30x30 output)**: 4x4 kernel, stride 1  \n",
        "    - $ \\text{RF} = 1 + (4 - 1) \\times 1 = 4 $\n",
        "\n",
        "2. **Layer Before Last**: (zero padding and leaky ReLU do not affect RF)\n",
        "    - $ \\text{RF} = 4 + (4 - 1) \\times 1 \\times 1 = 7 $\n",
        "\n",
        "3. **32x32 Layer**: 4x4 kernel, stride 2\n",
        "    - $ \\text{RF} = 7 + (4 - 1) \\times 1 \\times 1\n",
        "     \\times 2 = 13 $\n",
        "\n",
        "4. **64x64 Layer**: 4x4 kernel, stride 2\n",
        "    - $ \\text{RF} = 13 + (4 - 1) \\times 1 \\times 1\n",
        "     \\times 2 \\times 2 = 25 $\n",
        "\n",
        "5. **128x128 Layer**: 4x4 kernel, stride 2\n",
        "    - $ \\text{RF} = 25 + (4 - 1) \\times 1 \\times 1\n",
        "     \\times 2 \\times 2 \\times 2 = 49 $\n",
        "\n",
        "Note: Zero padding layers, batch normalization, and activation functions (ReLU, Leaky ReLU) do not affect the receptive field size, so they are omitted in the calculation.\n",
        "\n",
        "For this dataset and with this architecture the portion of each grid is 49*49 of the original image."
      ],
      "metadata": {
        "id": "6X8TOojGkNGt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss functions and the optimizer\n",
        "\n",
        "* **Discriminator loss**\n",
        "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
        "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
        "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
        "  * Then the total_loss is the sum of real_loss and the generated_loss\n",
        "  \n",
        "  The loss in the difference of the input and the label provided for that.  \n",
        "  real image > label=1  \n",
        "  generated image > label=0  \n",
        "  You can still use binary cross-entropy as your loss function even for a PatchGAN discriminator, because the binary cross-entropy can be applied to each patch independently.\n",
        "\n",
        "* **Generator loss**\n",
        "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
        "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
        "  * This allows the generated image to become structurally similar to the target image.\n",
        "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyhxTuvJyIHV"
      },
      "outputs": [],
      "source": [
        "LAMBDA = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "outputs": [],
      "source": [
        "# disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "def discriminator_loss(real_image, generated_image):\n",
        "  # from_logits > the output is the raw, unnormalized scores output by a model, can be any real number, positive, negative, or zero.\n",
        "  # Probabilities: Values between 0 and 1 obtained by applying the sigmoid function to the logits, representing the model’s confidence in a particular class.\n",
        "  loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "  # Compute the loss\n",
        "  # tf.ones_like(disc_real_output) creates a tensor that has the same shape as disc_real_output but is filled with ones.\n",
        "  # computes the Binary Cross-Entropy loss between the true labels (all ones) and the predicted labels.\n",
        "  # The total loss is the average of all these individual losses.\n",
        "  # real_image > b*256*256*3\n",
        "  # tf.one_like > b*256*256*3 > put the label one for each pixel\n",
        "  # OR 30*30*1 >> it is correct\n",
        "  real_loss = loss_fn(tf.ones_like(real_image), real_image)\n",
        "  generated_loss = loss_fn(tf.zeros_like(generated_image), generated_image)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "outputs": [],
      "source": [
        "# gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "def generator_loss(generated_image, gen_output, target):\n",
        "  loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "  gan_loss = loss_fn(tf.ones_like(generated_image), generated_image)\n",
        "\n",
        "  # mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.optimizers.Adam(2e-4, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKUZnDiqQrAh"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJnftd5sQsv6"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "# creating object to handle saving and restoring the state of the models and their corresponding optimizers.\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Training\n",
        "\n",
        "* We start by iterating over the dataset\n",
        "* The generator gets the input image and we get a generated output.\n",
        "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
        "* Next, we calculate the generator and the discriminator loss.\n",
        "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
        "\n",
        "## Generate Images\n",
        "\n",
        "* After training, its time to generate some images!\n",
        "* We pass images from the test dataset to the generator.\n",
        "* The generator will then translate the input image into the output we expect.\n",
        "* Last step is to plot the predictions and **voila!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "outputs": [],
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  # the training=True is intentional here since\n",
        "  # we want the batch statistics while running the model\n",
        "  # on the test dataset. If we use training=False, we will get\n",
        "  # the accumulated statistics learned from the training dataset\n",
        "  # (which we don't want)\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for input_image, target in dataset:\n",
        "\n",
        "      # When you’re training a model, you use a technique called backpropagation, which requires you to\n",
        "      # compute gradients or partial derivatives of the loss function with respect to the model parameters (or weights).\n",
        "      # This line is creating two separate gradient tapes: gen_tape and disc_tape. gen_tape will record\n",
        "      # operations for the generator, and disc_tape will record operations for the discriminator.\n",
        "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(input_image, training=True)\n",
        "\n",
        "        # now disriminative updates two times, first with the real data, second with the generated one\n",
        "        disc_real_output = discriminator(input_image, target, training=True)\n",
        "        disc_generated_output = discriminator(input_image, gen_output, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "      # for calculating GD we can exit the block\n",
        "      # This calculates the gradients of the losses with respect to the trainable variables\n",
        "      generator_gradients = gen_tape.gradient(gen_loss,\n",
        "                                              generator.trainable_variables)\n",
        "      discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                                   discriminator.trainable_variables)\n",
        "\n",
        "      # updating the weights of the generator and the discriminator.\n",
        "      generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                              generator.trainable_variables))\n",
        "      discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                                  discriminator.trainable_variables))\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        clear_output(wait=True)\n",
        "        for inp, tar in test_dataset.take(1):\n",
        "          generate_images(generator, inp, tar)\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1zZmKmvOH85"
      },
      "outputs": [],
      "source": [
        "train(train_dataset, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz80bY3aQ1VZ"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t4x69adQ5xb"
      },
      "outputs": [],
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RGysMU_BZhx"
      },
      "source": [
        "## Testing on the entire test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUgSnmy2nqSP"
      },
      "outputs": [],
      "source": [
        "# Run the trained model on the entire test dataset\n",
        "for inp, tar in test_dataset.take(5):\n",
        "  generate_images(generator, inp, tar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EihjPxFNYyo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}